{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52800281-517b-4a5e-a48f-1b8f9e4eed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ba1a3-11c9-4434-9686-09feb4696690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img  = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (150, 150))\n",
    "    img=  img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf15f460-c53c-4b14-8ae4-328ba69597ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"Images\"\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da82d10-cd55-46b3-8dfd-fdee5bf3415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270d5c23-656a-4113-9d18-8423e5d76dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_input = layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(img_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=RMSprop(learning_rate=0.001),\n",
    "            metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3741c80-b956-4385-a171-0245c770f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91798\\AppData\\Local\\Temp\\ipykernel_16408\\676592146.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 - 11s - loss: 0.9862 - acc: 0.3889 - val_loss: 0.2784 - val_acc: 0.3750 - 11s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "5/5 - 6s - loss: 0.3479 - acc: 0.3222 - val_loss: 0.2339 - val_acc: 0.2875 - 6s/epoch - 1s/step\n",
      "Epoch 3/10\n",
      "5/5 - 5s - loss: -4.7173e-03 - acc: 0.3333 - val_loss: -2.1593e+00 - val_acc: 0.3125 - 5s/epoch - 1s/step\n",
      "Epoch 4/10\n",
      "5/5 - 5s - loss: -7.8024e-01 - acc: 0.3600 - val_loss: -1.3301e+00 - val_acc: 0.3750 - 5s/epoch - 1s/step\n",
      "Epoch 5/10\n",
      "5/5 - 5s - loss: -1.0116e+01 - acc: 0.3111 - val_loss: -7.6614e+00 - val_acc: 0.6000 - 5s/epoch - 1s/step\n",
      "Epoch 6/10\n",
      "5/5 - 5s - loss: -1.6191e+01 - acc: 0.3889 - val_loss: -6.2217e+01 - val_acc: 0.3375 - 5s/epoch - 1s/step\n",
      "Epoch 7/10\n",
      "5/5 - 5s - loss: -5.8167e+01 - acc: 0.3778 - val_loss: -1.1212e+02 - val_acc: 0.3125 - 5s/epoch - 1s/step\n",
      "Epoch 8/10\n",
      "5/5 - 5s - loss: -1.2311e+02 - acc: 0.3222 - val_loss: -2.4423e+02 - val_acc: 0.4875 - 5s/epoch - 1s/step\n",
      "Epoch 9/10\n",
      "5/5 - 6s - loss: -2.5132e+02 - acc: 0.3700 - val_loss: -6.8900e+02 - val_acc: 0.2625 - 6s/epoch - 1s/step\n",
      "Epoch 10/10\n",
      "5/5 - 5s - loss: -3.0220e+02 - acc: 0.3889 - val_loss: -5.6225e+02 - val_acc: 0.3125 - 5s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=4,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d3402-5d88-4ea5-baa8-7de5ca32821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join('Images', 'Train')):\n",
    "    for image in os.listdir(os.path.join('Images', 'Train', file)):\n",
    "        input_img = preprocess(os.path.join('Images/Input_Image', 'input_image.jpeg'))\n",
    "        validation_img = preprocess(os.path.join('Images', 'Train', file, image))\n",
    "results = model.evaluate(input_img, validation_img, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9bdb9ab-1526-4f16-8b05-a98039d9466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(str(file_path))\n",
    "    # img  = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (150, 150))\n",
    "    img=  img / 255.0\n",
    "    return img\n",
    "\n",
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Build results array\n",
    "    results = []\n",
    "    for file in os.listdir(os.path.join('Images', 'Train')):\n",
    "        for image in os.listdir(os.path.join('Images', 'Train', file)):\n",
    "            # Images/Input_Image/input_image.jpeg\n",
    "            input_img = preprocess(os.path.join('Images/Input_Image', 'input_image.jpeg'))\n",
    "            validation_img = preprocess(os.path.join('Images', 'Train', file, image))\n",
    "\n",
    "        # input_img = tf.image.resize(input_img, (100, 100))\n",
    "        # input_img=  input_img / 255.0\n",
    "        \n",
    "        # validation_img = tf.image.resize(validation_img, (100, 100))\n",
    "        # validation_img=  validation_img / 255.0\n",
    "        \n",
    "        # Make Predictions\n",
    "        result_input_img = model.predict(list(np.expand_dims([input_img], axis=1)))\n",
    "        result_validation_img = model.predict(list(np.expand_dim([input_img]), axis=1))\n",
    "        # results.append(result)\n",
    "        print(result_input_img, result_validation_img)\n",
    "\n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive\n",
    "    detection = np.sum(np.array(result) > detection_threshold)\n",
    "    print(detection) # 3 \n",
    "\n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples\n",
    "    verification = detection / len(os.listdir(os.path.join(\"Images\", 'Train', 'Bharat')))\n",
    "    print(verification) # 0.02\n",
    "    verified = verification > verification_threshold\n",
    "\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f55348-b960-4b4b-b32e-6b9c3753f1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     22\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput_Image\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_image.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m), frame)\n\u001b[1;32m---> 23\u001b[0m         results, verified \u001b[38;5;241m=\u001b[39m verify(\u001b[43mmodel\u001b[49m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(verified)\n\u001b[0;32m     26\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "pTime = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame from webcam.\")\n",
    "        break\n",
    "    # FPS\n",
    "    cTime = time.time()\n",
    "    fps = 1/ (cTime - pTime)\n",
    "    pTime = cTime\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (20, 70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)\n",
    "\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    if cv2.waitKey(1) == ord('v'):\n",
    "        cv2.imwrite(os.path.join(\"Images\", \"Input_Image\", \"input_image.jpeg\"), frame)\n",
    "        results, verified = verify(model, 0.9, 0.01)\n",
    "        print(verified)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e4a6620-4e48-43fe-b45d-f829f1d94a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[0.09789542, 0.09933333, 0.08717647],\n",
       "          [0.10905883, 0.10905883, 0.11611765],\n",
       "          [0.10424836, 0.10424836, 0.1120915 ],\n",
       "          ...,\n",
       "          [0.12222207, 0.1300652 , 0.1183005 ],\n",
       "          [0.12662773, 0.12584312, 0.12623543],\n",
       "          [0.1495684 , 0.13070562, 0.14614354]],\n",
       "\n",
       "         [[0.09777778, 0.09777778, 0.09490196],\n",
       "          [0.10941177, 0.10941177, 0.11647059],\n",
       "          [0.10313725, 0.10313725, 0.11098039],\n",
       "          ...,\n",
       "          [0.12666667, 0.1345098 , 0.1227451 ],\n",
       "          [0.12482372, 0.13188225, 0.12835298],\n",
       "          [0.1340263 , 0.13115023, 0.1404314 ]],\n",
       "\n",
       "         [[0.11176471, 0.10888889, 0.1103268 ],\n",
       "          [0.10392157, 0.10392157, 0.11176471],\n",
       "          [0.10915032, 0.10915032, 0.11699346],\n",
       "          ...,\n",
       "          [0.1232027 , 0.1232027 , 0.11535956],\n",
       "          [0.12960792, 0.13745105, 0.13352948],\n",
       "          [0.13333334, 0.13333334, 0.14117648]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.08254901, 0.08183007, 0.07398693],\n",
       "          [0.08215687, 0.08215687, 0.07784314],\n",
       "          [0.08333334, 0.09117647, 0.0872549 ],\n",
       "          ...,\n",
       "          [0.08464044, 0.08071887, 0.08267966],\n",
       "          [0.08803914, 0.08411758, 0.08607836],\n",
       "          [0.08111118, 0.08111118, 0.08111118]],\n",
       "\n",
       "         [[0.07986929, 0.07986929, 0.07202614],\n",
       "          [0.08117652, 0.08117652, 0.07333338],\n",
       "          [0.07908497, 0.07908497, 0.07124183],\n",
       "          ...,\n",
       "          [0.08117652, 0.08117652, 0.08117652],\n",
       "          [0.07713726, 0.07713726, 0.07713726],\n",
       "          [0.07742473, 0.08526786, 0.0813463 ]],\n",
       "\n",
       "         [[0.0761961 , 0.08403923, 0.07227453],\n",
       "          [0.08270591, 0.08270591, 0.07486277],\n",
       "          [0.08169934, 0.08169934, 0.0738562 ],\n",
       "          ...,\n",
       "          [0.07843138, 0.07843138, 0.07843138],\n",
       "          [0.07733306, 0.07803919, 0.07768612],\n",
       "          [0.07450981, 0.08235294, 0.07843138]]]]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = preprocess(os.path.join('Images/Input_Image', 'input_image.jpeg'))\n",
    "np.expand_dims([input_img], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fad5b-13e8-479e-829d-cd5187debd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
