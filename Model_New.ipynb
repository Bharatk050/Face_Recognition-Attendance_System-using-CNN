{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92db7460-a043-45ac-9e14-af33e41698a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from mtcnn import MTCNN\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8744a-4437-4df3-aa06-d73ede9cdc92",
   "metadata": {},
   "source": [
    "# Face Detection and Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9140c08d-fb6b-4ced-a79e-25172f598ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# input_faces = face_classifier.detectMultiScale(gray, 1.1, 5)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m input_faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m input_faces:\n\u001b[0;32m     19\u001b[0m     x, y, width, height \u001b[38;5;241m=\u001b[39m face[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\mtcnn\\mtcnn.py:300\u001b[0m, in \u001b[0;36mMTCNN.detect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# We pipe here each of the stages\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m stages:\n\u001b[1;32m--> 300\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m [total_boxes, points] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    304\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\mtcnn\\mtcnn.py:342\u001b[0m, in \u001b[0;36mMTCNN.__stage1\u001b[1;34m(self, image, scales, stage_status)\u001b[0m\n\u001b[0;32m    339\u001b[0m img_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(scaled_image, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    340\u001b[0m img_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(img_x, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m--> 342\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m out0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(out[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    345\u001b[0m out1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(out[\u001b[38;5;241m1\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:1059\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1058\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m-> 1059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training_arrays_v1.py:801\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[0;32m    798\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_standardize_user_data(\n\u001b[0;32m    799\u001b[0m     x, check_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39msteps\n\u001b[0;32m    800\u001b[0m )\n\u001b[1;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training_arrays_v1.py:421\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[0;32m    417\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[0;32m    418\u001b[0m )\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    423\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\keras\\src\\backend.py:4607\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4599\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4603\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[0;32m   4604\u001b[0m ):\n\u001b[0;32m   4605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4607\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[0;32m   4609\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4611\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4612\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4613\u001b[0m )\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1505\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1504\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1505\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1508\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1509\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "id = 1\n",
    "name = \"Bharat\"\n",
    "img_id = 41\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame from webcam.\")\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # input_faces = face_classifier.detectMultiScale(gray, 1.1, 5)\n",
    "    input_faces = detector.detect_faces(rgb_frame)\n",
    "\n",
    "\n",
    "    for face in input_faces:\n",
    "        x, y, width, height = face['box']\n",
    "        cropped_face = gray[y:y+height, x:x+width]\n",
    "        # cv2.rectangle(image_rgb, (x, y), (x + width, y + height), (0, 155, 255), 2)\n",
    "        img_id += 1\n",
    "        face = cv2.resize(cropped_face, (150, 150))\n",
    "        # file_name_path = f\"DataSet/{name}\" + \".\" + str(id) + \".\"+ str(img_id) + \".jpg\"\n",
    "        file_name_path = f\"visualization/{name}\" + \".\" + str(id) + \".\"+ str(img_id) + \".jpg\"\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "        cv2.putText(face, str(img_id), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "    # for (x, y, w, h) in input_faces:\n",
    "    #     cropped_face = frame[y:y+h, x:x+w]\n",
    "    #     img_id += 1\n",
    "    #     face = cv2.resize(cropped_face, (150, 150))\n",
    "    #     file_name_path = f\"DataSet/{name}\" + \".\" + str(id) + \".\"+ str(img_id) + \".jpg\"\n",
    "    #     cv2.imwrite(file_name_path, face)\n",
    "    #     cv2.putText(face, str(img_id), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Cropped Face\", face)\n",
    "        if cv2.waitKey(1) == ord('q') or img_id == 20:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"DataSet Successfully Generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5d491a-82bf-4002-ac2a-4bfb1d5b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_label(image_name):\n",
    "    name = image_name.split('.')[0]\n",
    "    if name == \"Bharat\":\n",
    "        return np.array([1,0,0])\n",
    "    elif name == \"Shyam\":\n",
    "        return np.array([0,1,0])\n",
    "    elif name == \"Khedup\":\n",
    "        return np.array([0,0,1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bf0b4-c5cd-4bd6-8af7-743928a63a44",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019e07a2-df2c-40ee-84d7-04f391c9506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data():\n",
    "    data= []\n",
    "    for img in tqdm(os.listdir(\"DataSet\")):\n",
    "        path = os.path.join(\"DataSet\", img)\n",
    "        img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data = cv2.resize(img_data, (50, 50))\n",
    "        data.append([np.array(img_data), my_label(img)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600a994-00ba-48ac-954d-8694ec09ffb1",
   "metadata": {},
   "source": [
    "# Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cd79c7-0f18-4944-8abf-88e7c78b3820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 1019.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 50, 50, 1)\n",
      "(60, 50, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = my_data()\n",
    "train = data[:240]\n",
    "test = data[240:]\n",
    "X_train = np.array([i[0] for i in train]).reshape(-1,50,50,1)\n",
    "y_train = np.array([i[1] for i in train])\n",
    "X_test = np.array([i[0] for i in test]).reshape(-1,50,50,1)\n",
    "y_test = np.array([i[1] for i in test])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4ae3d3-ba06-4f2c-823b-e8f1dab9806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train: print(np.argmax(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e72bc-3b51-4189-b8b3-1a0831b1528d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc7677c-95a0-48c0-bf4e-e49dbfa052ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\tflearn\\__init__.py:5: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From D:\\Python\\Lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image as pil\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "if parse_version(pil.__version__)>=parse_version('10.0.0'):\n",
    "    pil.ANTIALIAS=pil.LANCZOS\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d762d25a-b9c2-4650-b99b-02847b99d1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.66344\u001b[0m\u001b[0m | time: 0.287s\n",
      "| Adam | epoch: 012 | loss: 0.66344 - acc: 0.8497 -- iter: 192/240\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.56395\u001b[0m\u001b[0m | time: 1.381s\n",
      "| Adam | epoch: 012 | loss: 0.56395 - acc: 0.8713 | val_loss: 0.05393 - val_acc: 0.9833 -- iter: 240/240\n",
      "--\n",
      "Training Accuracy: []\n",
      "Validation Accuracy: []\n",
      "Training Loss: []\n",
      "Validation Loss: []\n"
     ]
    }
   ],
   "source": [
    "class MetricsCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_epoch_end(self, training_state):\n",
    "        self.train_acc.append(training_state.acc_value)\n",
    "        self.val_acc.append(training_state.val_acc_value)\n",
    "        self.train_loss.append(training_state.loss_value)\n",
    "        self.val_loss.append(training_state.val_loss_value)\n",
    "tf.compat.v1.reset_default_graph()\n",
    "convnet = input_data(shape=[50, 50, 1])\n",
    "convnet = conv_2d(convnet, 32, 5, activation= 'relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 3, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate = 0.001, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(convnet, tensorboard_verbose=1)\n",
    "metrics_callback = MetricsCallback()\n",
    "model.fit(X_train, y_train, n_epoch=12, validation_set=(X_test, y_test), show_metric = True, run_id=\"FRS\" )\n",
    "train_acc = metrics_callback.train_acc\n",
    "val_acc = metrics_callback.val_acc\n",
    "train_loss = metrics_callback.train_loss\n",
    "val_loss = metrics_callback.val_loss\n",
    "print(\"Training Accuracy:\", train_acc)\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "print(\"Training Loss:\", train_loss)\n",
    "print(\"Validation Loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbf36f9e-316a-479e-b530-9ead5f552027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10c9191e-f79c-4977-b234-c71e8e892fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_callback = MetricsCallback()\n",
    "train_acc = metrics_callback.train_acc\n",
    "val_acc = metrics_callback.val_acc\n",
    "train_loss = metrics_callback.train_loss\n",
    "val_loss = metrics_callback.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdd6b867-9774-47dc-a030-fedddc40a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaa1ee-a95b-4aca-82b7-353d59ec0274",
   "metadata": {},
   "source": [
    "# Feature Engineering for Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "832b0f55-9eff-403b-b07c-98d01477ec10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\n\u001b[0;32m      2\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mval_accuracy\n\u001b[0;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'accuracy'"
     ]
    }
   ],
   "source": [
    "acc = history.accuracy\n",
    "val_acc = history.val_accuracy\n",
    "\n",
    "loss = history.loss\n",
    "val_loss = history.val_loss\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82c5d18-6840-49f7-877e-7721986cc05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_visualization():\n",
    "    Vdata = []\n",
    "    for img in tqdm(os.listdir(\"visualization\")):\n",
    "        path = os.path.join(\"visualization\", img)\n",
    "        img_num = img.split('.')[0] \n",
    "        img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data = cv2.resize(img_data, (50,50))\n",
    "        Vdata.append([np.array(img_data), img_num])\n",
    "    shuffle(Vdata)\n",
    "    return Vdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a6d1df4-88a2-4565-9e29-ece72598be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 2349.53it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be an integer with 1 <= num <= 1, not 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Vdata[\u001b[38;5;241m0\u001b[39m:]):\n\u001b[0;32m      5\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_subplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     image \u001b[38;5;241m=\u001b[39m img_data\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m img_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\matplotlib\\figure.py:782\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])))\n\u001b[0;32m    781\u001b[0m     projection_class, pkw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_projection_requirements(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 782\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[43mprojection_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     key \u001b[38;5;241m=\u001b[39m (projection_class, pkw)\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_axes_internal(ax, key)\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\matplotlib\\axes\\_base.py:639\u001b[0m, in \u001b[0;36m_AxesBase.__init__\u001b[1;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_originalPosition \u001b[38;5;241m=\u001b[39m mtransforms\u001b[38;5;241m.\u001b[39mBbox\u001b[38;5;241m.\u001b[39munit()\n\u001b[1;32m--> 639\u001b[0m     subplotspec \u001b[38;5;241m=\u001b[39m \u001b[43mSubplotSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_subplot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWidth and height specified must be non-negative\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\matplotlib\\gridspec.py:599\u001b[0m, in \u001b[0;36mSubplotSpec._from_subplot_args\u001b[1;34m(figure, args)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num, Integral) \u001b[38;5;129;01mor\u001b[39;00m num \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m num \u001b[38;5;241m>\u001b[39m rows\u001b[38;5;241m*\u001b[39mcols:\n\u001b[1;32m--> 599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum must be an integer with 1 <= num <= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrows\u001b[38;5;241m*\u001b[39mcols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    601\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    602\u001b[0m         )\n\u001b[0;32m    603\u001b[0m     i \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m=\u001b[39m num\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gs[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:j]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be an integer with 1 <= num <= 1, not 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt   # installation command: pip install matplotlib\n",
    "Vdata = data_for_visualization()\n",
    "fig = plt.figure(figsize=(6,3))\n",
    "for num, data in enumerate(Vdata[0:]):\n",
    "    img_data = data[0]\n",
    "    y = fig.add_subplot(1,1, num+1)\n",
    "    image = img_data\n",
    "    data = img_data.reshape(50,50,1)\n",
    "    model_out = model.predict([data])[0]\n",
    "    # print(model_out)\n",
    "     \n",
    "    if np.argmax(model_out) == 0:\n",
    "        my_label = 'Bharat'\n",
    "    elif np.argmax(model_out) == 1:\n",
    "        my_label = 'Shyam'\n",
    "    else:\n",
    "        my_label = 'Khedup'\n",
    "\n",
    "    # f = plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(image, cmap= 'gray')\n",
    "    plt.title(my_label)\n",
    "    \n",
    "    file_path = 'Attendance_Log.xlsx'\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    sheet = workbook.active \n",
    "    # Update a cell value\n",
    "    sheet['A1'] = f'{my_label}'\n",
    "    sheet['B1'] = 'Present'\n",
    "    workbook.save(file_path)\n",
    "    \n",
    "    y.axes.get_xaxis().set_visible(False)\n",
    "    y.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ee643-b951-4822-8e6f-88bca4ec4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7770e-5f6b-4afe-83b7-e8f0c1f478e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bee74-076e-438b-a3ab-2b2a2f31cc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf27507-a6c1-42f3-bacd-352deeb2539c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
